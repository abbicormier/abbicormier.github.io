"Application Deadline",
"JEL Classifications",
"Location",
"Country",
"Citizenship Requirements",
"Review Date",
"Application Requirements",
"JOE Link",
"Full Job Description",
"Preferred Deadline",
],
)
df.to_excel("JOE_jobs_refactored.xlsx")
print("*******************DONE*************************")
import pandas as pd
import re
import time
import dateparser
import spacy
import requests
from bs4 import BeautifulSoup
# Load spaCy's small English model
nlp = spacy.load("en_core_web_sm")
def get_job_description(url):
response = requests.get(url)
soup = BeautifulSoup(response.text, "lxml")
job_description_container = soup.find("p", class_="full-text")
if job_description_container:
job_description = job_description_container.text.strip()
return job_description
else:
print("Job description not found.")
return None
def get_job_listings(url):
# Get the HTML content and parse with BeautifulSoup
response = requests.get(url)
response.encoding = "utf-8"
soup = BeautifulSoup(response.text, "lxml")
# Find all listings
results = soup.findAll("div", attrs={"class": "listing-institution-group-item"})
return results
def extract_job_details(result):
job_list = []
inst = result.find("h5", attrs={"class": "group-header-title"}).text
titles = result.findAll("h6", attrs={"class": "listing-item-header-title"})
posted = result.findAll("div", attrs={"class": "listing-item-header-date-posted"})
bodies = result.findAll("div", attrs={"class": "listing-item-body"})
for i in range(len(titles)):
pos = titles[i].find("a").text
joelink = (
"https://www.aeaweb.org/" + (titles[i].find("a")["href"]).split("&")[0]
)
try:
deadline = (
bodies[i]
.find(
"div", attrs={"class": "application-deadline app-instruct-deadline"}
)
.text
)
except Exception as e:
print(f"Error extracting deadline: {e}")
deadline = None
location = ((bodies[i].findAll("h6")[1].text).split(":")[1]).strip()
jel_descr = bodies[i].findAll("h6")[2]
jels = jel_descr.find_next_siblings("div", attrs={"class": "meta-list-item"})
jel_list = (jel_descr.text).split(":")[1].strip()
for jcode in jels:
jel_list += jcode.text + ","
country = location.split(",")[-1].strip()
requirements, citizen, review = get_additional_job_info(joelink)
date_posted = (posted[i].text).split("Date Posted: ")[1]
app_deadline = deadline.split("Application deadline: ")[1] if deadline else None
fulljobdescription = get_job_description(joelink)
preferred_deadline = extract_preferred_deadline(fulljobdescription)
job_list.append(
[
inst,
pos,
date_posted,
app_deadline,
jel_list,
location,
country,
citizen,
review,
requirements,
joelink,
fulljobdescription,
preferred_deadline,
]
)
# Add delay between requests
time.sleep(1)
return job_list
def get_additional_job_info(url):
# Request for full job description and extract requirements, citizenship, and review info
website = requests.get(url).text
soup2 = BeautifulSoup(website, "lxml")
try:
req = soup2.find("ul", attrs={"class": "app-instruct-desc"}).find_all("li")
requirements = ",".join([r.text for r in req])
except:
requirements = ""
cit = soup2.find("p", attrs={"class": "full-text"}).text
cit = cit.replace("U.S.", "US")
citizen = "".join(re.findall(r"([^.]*?citizen[^.]*\.)", cit))
review = "".join(re.findall(r"([^.]*?review[^.]*\.)", cit))
review = "" if "peer-review" in review else review
return requirements, citizen, review
# Define a function to extract "preferred deadline"
def extract_preferred_deadline(text):
# Step 1: Preprocess text with spaCy NLP model
doc = nlp(text)
# Step 2: Use regex to search for phrases indicating a preferred deadline
preferred_deadline_patterns = [
r"preferred deadline is (\w+ \d{1,2}, \d{4})",
r"preferred application deadline is (\w+ \d{1,2}, \d{4})",
r"preferred deadline.*?(\w+ \d{1,2}, \d{4})",
r"initial deadline is (\w+ \d{1,2}, \d{4})",
r"initial application deadline is (\w+ \d{1,2}, \d{4})",
r"initial deadline.*?(\w+ \d{1,2}, \d{4})",
r"by (\w+ \d{1,2}, \d{4})",
r"by the (\w+ \d{1,2}, \d{4})",
r"by.*?(\w+ \d{1,2}, \d{4})",
r"should be submitted by the (\w+ \d{1,2}, \d{4})",
r"should be submitted by.*?(\w+ \d{1,2}, \d{4})",
]
preferred_deadline = None
for pattern in preferred_deadline_patterns:
match = re.search(pattern, text, re.IGNORECASE)
if match:
preferred_deadline = match.group(1)
break
# Step 3: Use spaCy NER to further verify and extract date entities
if not preferred_deadline:
for ent in doc.ents:
if ent.label_ == "DATE":
if "preferred" in text[ent.start_char - 20 : ent.start_char].lower():
preferred_deadline = ent.text
break
# Step 4: Parse the date if necessary to ensure consistency
if preferred_deadline:
parsed_date = dateparser.parse(preferred_deadline)
if parsed_date:
return parsed_date.strftime("%B %d, %Y")
else:
return "not found"
# Main Script
url = "https://www.aeaweb.org/joe/listings"
results = get_job_listings(url)
jobs = []
for result in results:
jobs.extend(extract_job_details(result))
df = pd.DataFrame(
jobs,
columns=[
"Institution",
"Position",
"Date Posted",
"Application Deadline",
"JEL Classifications",
"Location",
"Country",
"Citizenship Requirements",
"Review Date",
"Application Requirements",
"JOE Link",
"Full Job Description",
"Preferred Deadline",
],
)
df.to_excel("JOE_jobs_refactored.xlsx")
print("*******************DONE*************************")
import pandas as pd
import re
import time
import dateparser
import spacy
import requests
from bs4 import BeautifulSoup
# Load spaCy's small English model
nlp = spacy.load("en_core_web_sm")
def get_job_description(url):
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")  # Changed from "lxml"
job_description_container = soup.find("p", class_="full-text")
if job_description_container:
job_description = job_description_container.text.strip()
return job_description
else:
print("Job description not found.")
return None
def get_job_listings(url):
# Get the HTML content and parse with BeautifulSoup
response = requests.get(url)
response.encoding = "utf-8"
soup = BeautifulSoup(response.text, "html.parser")  # Changed from "lxml"
# Find all listings
results = soup.findAll("div", attrs={"class": "listing-institution-group-item"})
return results
def extract_job_details(result):
job_list = []
inst = result.find("h5", attrs={"class": "group-header-title"}).text
titles = result.findAll("h6", attrs={"class": "listing-item-header-title"})
posted = result.findAll("div", attrs={"class": "listing-item-header-date-posted"})
bodies = result.findAll("div", attrs={"class": "listing-item-body"})
for i in range(len(titles)):
pos = titles[i].find("a").text
joelink = (
"https://www.aeaweb.org/" + (titles[i].find("a")["href"]).split("&")[0]
)
try:
deadline = (
bodies[i]
.find(
"div", attrs={"class": "application-deadline app-instruct-deadline"}
)
.text
)
except Exception as e:
print(f"Error extracting deadline: {e}")
deadline = None
location = ((bodies[i].findAll("h6")[1].text).split(":")[1]).strip()
jel_descr = bodies[i].findAll("h6")[2]
jels = jel_descr.find_next_siblings("div", attrs={"class": "meta-list-item"})
jel_list = (jel_descr.text).split(":")[1].strip()
for jcode in jels:
jel_list += jcode.text + ","
country = location.split(",")[-1].strip()
requirements, citizen, review = get_additional_job_info(joelink)
date_posted = (posted[i].text).split("Date Posted: ")[1]
app_deadline = deadline.split("Application deadline: ")[1] if deadline else None
fulljobdescription = get_job_description(joelink)
preferred_deadline = extract_preferred_deadline(fulljobdescription)
job_list.append(
[
inst,
pos,
date_posted,
app_deadline,
jel_list,
location,
country,
citizen,
review,
requirements,
joelink,
fulljobdescription,
preferred_deadline,
]
)
# Add delay between requests
time.sleep(1)
return job_list
def get_additional_job_info(url):
# Request for full job description and extract requirements, citizenship, and review info
website = requests.get(url).text
soup2 = BeautifulSoup(website, "html.parser")  # Changed from "lxml"
try:
req = soup2.find("ul", attrs={"class": "app-instruct-desc"}).find_all("li")
requirements = ",".join([r.text for r in req])
except:
requirements = ""
cit = soup2.find("p", attrs={"class": "full-text"}).text
cit = cit.replace("U.S.", "US")
citizen = "".join(re.findall(r"([^.]*?citizen[^.]*\.)", cit))
review = "".join(re.findall(r"([^.]*?review[^.]*\.)", cit))
review = "" if "peer-review" in review else review
return requirements, citizen, review
# Define a function to extract "preferred deadline"
def extract_preferred_deadline(text):
if not text:  # Handle case where text is None
return None
# Step 1: Preprocess text with spaCy NLP model
doc = nlp(text)
# Step 2: Use regex to search for phrases indicating a preferred deadline
preferred_deadline_patterns = [
r"preferred deadline is (\w+ \d{1,2}, \d{4})",
r"preferred application deadline is (\w+ \d{1,2}, \d{4})",
r"preferred deadline.*?(\w+ \d{1,2}, \d{4})",
r"initial deadline is (\w+ \d{1,2}, \d{4})",
r"initial application deadline is (\w+ \d{1,2}, \d{4})",
r"initial deadline.*?(\w+ \d{1,2}, \d{4})",
r"by (\w+ \d{1,2}, \d{4})",
r"by the (\w+ \d{1,2}, \d{4})",
r"by.*?(\w+ \d{1,2}, \d{4})",
r"should be submitted by the (\w+ \d{1,2}, \d{4})",
r"should be submitted by.*?(\w+ \d{1,2}, \d{4})",
]
preferred_deadline = None
for pattern in preferred_deadline_patterns:
match = re.search(pattern, text, re.IGNORECASE)
if match:
preferred_deadline = match.group(1)
break
# Step 3: Use spaCy NER to further verify and extract date entities
if not preferred_deadline:
for ent in doc.ents:
if ent.label_ == "DATE":
if "preferred" in text[ent.start_char - 20 : ent.start_char].lower():
preferred_deadline = ent.text
break
# Step 4: Parse the date if necessary to ensure consistency
if preferred_deadline:
parsed_date = dateparser.parse(preferred_deadline)
if parsed_date:
return parsed_date.strftime("%B %d, %Y")
else:
return "not found"
return None
# Main Script
url = "https://www.aeaweb.org/joe/listings?q=eNplUMFKxEAM_ZecK8wW9dDbgggrCwriSWSYTmMdzWZKZloppf9uxu3iwcMLk5fkvUwWOIaUA_fpPsoJmgUCW-dzmBAaMFDBF87fUTqb0In_UFK5hNoRGZpX2GlauxJaDTeKW3ir4BOpVE1R2CvuFAfFg-KoeCxNIaWx2NSmvr4ytdJRQh_Y0eFfxceRs8xWsD87L3B5gql3fx2avzwXy5RdxlwniVNgX7QcEazqmtyEnX2P1KEkaHgk0mHHXeh0wCYv7tQSXiqCHjnbyDT_usbh_HXYb3q0HdDqlVBsWzYIrFxcHkunLcehYdg2qGBw_Sa-rj8tXXkR"
print("Starting job scraping...")
results = get_job_listings(url)
print(f"Found {len(results)} institution groups")
jobs = []
for result in results:
jobs.extend(extract_job_details(result))
print(f"Processed jobs, total so far: {len(jobs)}")
print(f"Creating DataFrame with {len(jobs)} jobs...")
df = pd.DataFrame(
jobs,
columns=[
"Institution",
"Position",
"Date Posted",
"Application Deadline",
"JEL Classifications",
"Location",
"Country",
"Citizenship Requirements",
"Review Date",
"Application Requirements",
"JOE Link",
"Full Job Description",
"Preferred Deadline",
],
)
print("Saving to Excel...")
df.to_excel("JOE_jobs_refactored.xlsx", index=False)
print("*******************DONE*************************")
print(f"Scraped {len(jobs)} jobs and saved to JOE_jobs_refactored.xlsx")
import pandas as pd
import re
import time
import dateparser
import spacy
import requests
from bs4 import BeautifulSoup
# Load spaCy's small English model
nlp = spacy.load("en_core_web_sm")
def get_job_description(url):
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")  # Changed from "lxml"
job_description_container = soup.find("p", class_="full-text")
if job_description_container:
job_description = job_description_container.text.strip()
return job_description
else:
print("Job description not found.")
return None
def get_job_listings(url):
# Get the HTML content and parse with BeautifulSoup
response = requests.get(url)
response.encoding = "utf-8"
soup = BeautifulSoup(response.text, "html.parser")  # Changed from "lxml"
# Find all listings
results = soup.findAll("div", attrs={"class": "listing-institution-group-item"})
return results
def extract_job_details(result):
job_list = []
inst = result.find("h5", attrs={"class": "group-header-title"}).text
titles = result.findAll("h6", attrs={"class": "listing-item-header-title"})
posted = result.findAll("div", attrs={"class": "listing-item-header-date-posted"})
bodies = result.findAll("div", attrs={"class": "listing-item-body"})
for i in range(len(titles)):
pos = titles[i].find("a").text
joelink = (
"https://www.aeaweb.org/" + (titles[i].find("a")["href"]).split("&")[0]
)
try:
deadline = (
bodies[i]
.find(
"div", attrs={"class": "application-deadline app-instruct-deadline"}
)
.text
)
except Exception as e:
print(f"Error extracting deadline: {e}")
deadline = None
location = ((bodies[i].findAll("h6")[1].text).split(":")[1]).strip()
jel_descr = bodies[i].findAll("h6")[2]
jels = jel_descr.find_next_siblings("div", attrs={"class": "meta-list-item"})
jel_list = (jel_descr.text).split(":")[1].strip()
for jcode in jels:
jel_list += jcode.text + ","
country = location.split(",")[-1].strip()
requirements, citizen, review = get_additional_job_info(joelink)
date_posted = (posted[i].text).split("Date Posted: ")[1]
app_deadline = deadline.split("Application deadline: ")[1] if deadline else None
fulljobdescription = get_job_description(joelink)
preferred_deadline = extract_preferred_deadline(fulljobdescription)
job_list.append(
[
inst,
pos,
date_posted,
app_deadline,
jel_list,
location,
country,
citizen,
review,
requirements,
joelink,
fulljobdescription,
preferred_deadline,
]
)
# Add delay between requests
time.sleep(1)
return job_list
def get_additional_job_info(url):
# Request for full job description and extract requirements, citizenship, and review info
website = requests.get(url).text
soup2 = BeautifulSoup(website, "html.parser")  # Changed from "lxml"
try:
req = soup2.find("ul", attrs={"class": "app-instruct-desc"}).find_all("li")
requirements = ",".join([r.text for r in req])
except:
requirements = ""
cit = soup2.find("p", attrs={"class": "full-text"}).text
cit = cit.replace("U.S.", "US")
citizen = "".join(re.findall(r"([^.]*?citizen[^.]*\.)", cit))
review = "".join(re.findall(r"([^.]*?review[^.]*\.)", cit))
review = "" if "peer-review" in review else review
return requirements, citizen, review
# Define a function to extract "preferred deadline"
def extract_preferred_deadline(text):
if not text:  # Handle case where text is None
return None
# Step 1: Preprocess text with spaCy NLP model
doc = nlp(text)
# Step 2: Use regex to search for phrases indicating a preferred deadline
preferred_deadline_patterns = [
r"preferred deadline is (\w+ \d{1,2}, \d{4})",
r"preferred application deadline is (\w+ \d{1,2}, \d{4})",
r"preferred deadline.*?(\w+ \d{1,2}, \d{4})",
r"initial deadline is (\w+ \d{1,2}, \d{4})",
r"initial application deadline is (\w+ \d{1,2}, \d{4})",
r"initial deadline.*?(\w+ \d{1,2}, \d{4})",
r"by (\w+ \d{1,2}, \d{4})",
r"by the (\w+ \d{1,2}, \d{4})",
r"by.*?(\w+ \d{1,2}, \d{4})",
r"should be submitted by the (\w+ \d{1,2}, \d{4})",
r"should be submitted by.*?(\w+ \d{1,2}, \d{4})",
]
preferred_deadline = None
for pattern in preferred_deadline_patterns:
match = re.search(pattern, text, re.IGNORECASE)
if match:
preferred_deadline = match.group(1)
break
# Step 3: Use spaCy NER to further verify and extract date entities
if not preferred_deadline:
for ent in doc.ents:
if ent.label_ == "DATE":
if "preferred" in text[ent.start_char - 20 : ent.start_char].lower():
preferred_deadline = ent.text
break
# Step 4: Parse the date if necessary to ensure consistency
if preferred_deadline:
parsed_date = dateparser.parse(preferred_deadline)
if parsed_date:
return parsed_date.strftime("%B %d, %Y")
else:
return "not found"
return None
# Main Script
url = "https://www.aeaweb.org/joe/listings"
print("Starting job scraping...")
results = get_job_listings(url)
print(f"Found {len(results)} institution groups")
jobs = []
for result in results:
jobs.extend(extract_job_details(result))
print(f"Processed jobs, total so far: {len(jobs)}")
print(f"Creating DataFrame with {len(jobs)} jobs...")
df = pd.DataFrame(
jobs,
columns=[
"Institution",
"Position",
"Date Posted",
"Application Deadline",
"JEL Classifications",
"Location",
"Country",
"Citizenship Requirements",
"Review Date",
"Application Requirements",
"JOE Link",
"Full Job Description",
"Preferred Deadline",
],
)
print("Saving to Excel...")
df.to_excel("JOE_jobs_refactored.xlsx", index=False)
print("*******************DONE*************************")
print(f"Scraped {len(jobs)} jobs and saved to JOE_jobs_refactored.xlsx")
